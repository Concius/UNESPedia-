# ğŸ”¬ UNESPedia

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue?logo=python" alt="Python Version" />
  <img src="https://img.shields.io/badge/Streamlit-Framework-red?logo=streamlit" alt="Framework" />
  <img src="https://img.shields.io/badge/Arquitetura-Modular-yellow" alt="Arquitetura Modular" />
  <img src="https://img.shields.io/badge/License-MIT-green" alt="License" />
</div>

<div align="center">
  <h3>Converse com Seus Artigos de Forma Inteligente</h3>
</div>

## ğŸ“– Sobre o Projeto

O **UNESPedia** Ã© uma aplicaÃ§Ã£o web desenvolvida como parte da disciplina de Aprendizado Profundo do PPGCC-Unesp. Ele utiliza a tÃ©cnica de **Retrieval-Augmented Generation (RAG)** para permitir que vocÃª converse com os seus prÃ³prios documentos, transformando artigos densos em diÃ¡logos interativos.

ConstruÃ­do com uma arquitetura modular e flexÃ­vel, o UNESPedia Ã© uma ferramenta poderosa para pesquisadores, estudantes e qualquer pessoa que precise extrair informaÃ§Ãµes e insights de uma base de documentos de forma rÃ¡pida e intuitiva.


## ğŸ—ï¸ Arquitetura do Sistema

### Stack TecnolÃ³gico

**Interface:**
- Streamlit - Framework web em Python para interfaces interativas

**Processamento de Documentos:**
- PyPDF - ExtraÃ§Ã£o de texto de PDFs
- Sentence-Transformers - Embeddings multilÃ­ngues (paraphrase-multilingual-mpnet-base-v2, 768 dimensÃµes)

**Armazenamento Vetorial:**
- ChromaDB - Persistente em disco, ideal para produÃ§Ã£o
- FAISS - Em memÃ³ria, otimizado para desenvolvimento

**Modelos LLM Suportados:**
- Google Gemini 2.5 Flash (1M tokens context window)
- Anthropic Claude Sonnet 4 (200K tokens context window)
- Deepseek Chat (64K tokens context window)
- OpenAI GPT (compatÃ­vel via API)

**Gerenciamento:**
- Chaves API: `secrets.json` (local, nÃ£o versionado)
- ConfiguraÃ§Ã£o: `config.yaml` (centralizado)
- HistÃ³rico: JSON persistente por conversa

### Pipeline RAG

1. **Upload de PDF** - UsuÃ¡rio carrega documentos
2. **ExtraÃ§Ã£o de Texto** - PyPDF processa os PDFs
3. **DivisÃ£o em Chunks** - Texto dividido com overlap configurÃ¡vel
4. **GeraÃ§Ã£o de Embeddings** - Modelo multilÃ­ngue cria vetores
5. **Armazenamento Vetorial** - ChromaDB ou FAISS indexa os chunks
6. **Query do UsuÃ¡rio** - Pergunta submetida
7. **Busca SemÃ¢ntica** - Top-k retrieval nos embeddings
8. **GeraÃ§Ã£o com Contexto** - LLM recebe chunks relevantes
9. **Resposta com CitaÃ§Ãµes** - Formato acadÃªmico (Fonte, p. X, sec. Y)

## âœ¨ Funcionalidades Principais


### ğŸ¤– RAG com CitaÃ§Ãµes AutomÃ¡ticas
- Sistema de busca semÃ¢ntica em mÃºltiplos documentos
- CitaÃ§Ãµes automÃ¡ticas no formato acadÃªmico: (Fonte, p. X, sec. Y)
- Top-k retrieval configurÃ¡vel para melhor contexto

### ğŸ”Œ Arquitetura Multi-LLM
- Troca entre modelos em tempo real com um simples seletor
- Suporte para mÃºltiplos provedores simultaneamente
- ConfiguraÃ§Ã£o individual por provedor (temperatura, top-p, top-k)

### âœï¸ System Prompt EditÃ¡vel
O aplicativo permite customizaÃ§Ã£o completa do comportamento do assistente atravÃ©s de um editor integrado.

**Prompt padrÃ£o:**
```
VocÃª Ã© um assistente de pesquisa acadÃªmica. 
Responda baseando-se APENAS no Contexto e HistÃ³rico fornecidos.

IMPORTANTE: sempre cite as fontes usando o formato:
(Fonte, p. {page}, sec. {section})

Se a informaÃ§Ã£o de pÃ¡gina ou seÃ§Ã£o nÃ£o estiver disponÃ­vel, omita esse campo.
```

**Recursos do editor:**
- Editor de texto completo na interface
- Preview com mÃ©tricas (tokens, caracteres, linhas)
- Exportar/importar configuraÃ§Ãµes
- Reset para padrÃ£o

### ğŸ­ Personas CustomizÃ¡veis
**7 personas prÃ©-definidas:**
1. Pesquisador AcadÃªmico
2. Professor UniversitÃ¡rio
3. Analista TÃ©cnico
4. Fact-Checker Rigoroso
5. Revisor de Literatura
6. Consultor de Pesquisa
7. Especialista em Metodologia

**Recursos:**
- Criar personas customizadas
- Editor de prompts por persona
- Salvar e carregar configuraÃ§Ãµes
- Gerenciar biblioteca de personas

### ğŸ‘¤ Perfil de Pesquisador
- ExtraÃ§Ã£o automÃ¡tica de metadados (autores, ano, tÃ­tulo, DOI)
- GeraÃ§Ã£o de perfil acadÃªmico via LLM
- Sistema de tags e categorizaÃ§Ã£o
- Biblioteca de perfis salvos com busca e filtros

### âš™ï¸ Presets de ConfiguraÃ§Ã£o
TrÃªs presets prÃ©-configurados para diferentes casos de uso:

**Preciso** (anÃ¡lises tÃ©cnicas):
- Temperature: 0.2
- Top-p: 0.9
- Top-k: 20

**Equilibrado** (uso geral):
- Temperature: 0.7
- Top-p: 0.95
- Top-k: 40

**Criativo** (brainstorming):
- Temperature: 1.2
- Top-p: 0.98
- Top-k: 50

Todos os parÃ¢metros sÃ£o ajustÃ¡veis manualmente pela interface.

### ğŸ’¬ GestÃ£o Completa de Conversas
- **Salvamento AutomÃ¡tico**: Cada conversa Ã© salva automaticamente em JSON
- **Carregar e Continuar**: Carregue conversas anteriores e continue de onde parou
- **Renomear e Organizar**: Organize as suas sessÃµes de chat diretamente pela interface
- **Apagar**: Remova conversas que nÃ£o precisa mais

### ğŸ”„ HistÃ³rico Interativo
- **Regenerar**: NÃ£o gostou da resposta? Gere uma nova com um clique
- **Editar**: Modifique qualquer mensagem (sua ou do assistente) para refinar o contexto
- **Apagar**: Remova mensagens individuais para limpar o histÃ³rico
- **Modo Debug**: Visualize os chunks recuperados e o processo de busca

## âš™ï¸ HiperparÃ¢metros


### Chunking
```
chunk_size: 512 tokens
chunk_overlap: 50 tokens
```

### Retrieval
```
n_results: 10 (top-k chunks recuperados)
```

### Embeddings
```
Modelo: paraphrase-multilingual-mpnet-base-v2
DimensÃ£o: 768
Device: CPU/CUDA/MPS (detecÃ§Ã£o automÃ¡tica)
```

### Context Window por Modelo
```
Gemini 2.5 Flash: 1M tokens
Claude Sonnet 4: 200K tokens
Deepseek Chat: 64K tokens
```

## ğŸš€ Como Executar

### 1ï¸âƒ£ PrÃ©-requisitos
- Python 3.8 ou superior
- pip (gerenciador de pacotes do Python)
- Git

### 2ï¸âƒ£ InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/Concius/UNESPedia-.git

# 2. Acesse a pasta do projeto
cd UNESPedia-

# 3. Instale todas as dependÃªncias
pip install -r requirements.txt

# 4. Crie o diretÃ³rio para salvar as conversas
mkdir chats
```

### 3ï¸âƒ£ ConfiguraÃ§Ã£o das Chaves de API

As chaves de API agora sÃ£o geridas de forma persistente:

1. Execute a aplicaÃ§Ã£o pela primeira vez: `streamlit run app.py`
2. Na barra lateral, selecione um provedor de LLM (ex: Gemini)
3. Cole a sua chave de API no campo correspondente
4. A chave serÃ¡ salva automaticamente no ficheiro `secrets.json`

> **âš ï¸ Importante**: O ficheiro `secrets.json` estÃ¡ incluÃ­do no `.gitignore` para garantir que as suas chaves nunca sejam enviadas para o repositÃ³rio.

### 4ï¸âƒ£ Executando a AplicaÃ§Ã£o

```bash
streamlit run app.py
```

A aplicaÃ§Ã£o serÃ¡ aberta automaticamente no seu navegador. Agora, basta carregar os seus PDFs, processÃ¡-los e comeÃ§ar a sua pesquisa!

## ğŸ› ï¸ Estrutura do Projeto


O cÃ³digo Ã© organizado de forma modular para facilitar a manutenÃ§Ã£o e a adiÃ§Ã£o de novas funcionalidades:

```
UNESPedia/
â”œâ”€â”€ app.py                 # Ponto de entrada da aplicaÃ§Ã£o (UI Streamlit)
â”œâ”€â”€ config.yaml           # ConfiguraÃ§Ãµes centrais (modelos, presets, etc.)
â”œâ”€â”€ rag_processor.py      # LÃ³gica de RAG (divisÃ£o de texto em chunks, embeddings)
â”œâ”€â”€ llm_providers/        # MÃ³dulos para cada provedor de LLM
â”‚   â”œâ”€â”€ gemini.py
â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”œâ”€â”€ claude.py
â”‚   â””â”€â”€ deepseek.py
â”œâ”€â”€ vector_stores/        # MÃ³dulos para bases de dados vetoriais
â”‚   â”œâ”€â”€ chroma_store.py
â”‚   â””â”€â”€ faiss_store.py
â”œâ”€â”€ chat_manager.py       # GestÃ£o de ficheiros de conversa
â”œâ”€â”€ secrets_manager.py    # GestÃ£o de chaves de API
â”œâ”€â”€ prompt_manager.py     # GestÃ£o de prompts e personas
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md
```

## ğŸ“Š AvaliaÃ§Ã£o Experimental

O sistema foi avaliado comparando **3 LLMs** (Gemini 2.5 Flash, Claude Sonnet 4, Deepseek) e **2 vector stores** (ChromaDB, FAISS) utilizando um dataset de 3 PDFs sobre seguranÃ§a e machine learning com 7 perguntas factuais.

### MÃ©tricas Avaliadas
- â±ï¸ Tempo de resposta total
- ğŸ“ Qualidade de citaÃ§Ãµes
- ğŸ’° Custo por consulta
- ğŸ—„ï¸ Tempo de indexaÃ§Ã£o

### ConfiguraÃ§Ã£o dos Testes
```
Temperature: 0.7
Top-p: 0.95
Top-k: 40
Max tokens: 2048
```

### Scripts DisponÃ­veis
Os scripts de avaliaÃ§Ã£o estÃ£o disponÃ­veis em `/avaliacao/`:
- `avaliar_sistema.py` - Script principal de testes automatizados
- `visualizar_resultados.py` - GeraÃ§Ã£o de grÃ¡ficos e anÃ¡lises
- `README_AVALIACAO.md` - DocumentaÃ§Ã£o completa da metodologia

## ğŸ“œ LicenÃ§a

**MIT License**

### PermissÃµes
- âœ… ModificaÃ§Ã£o
- âœ… DistribuiÃ§Ã£o
- âœ… Uso privado

**Uso livre e irrestrito pela comunidade Unesp** (alunos, professores e funcionÃ¡rios).

## ğŸ’¡ SugestÃµes & ContribuiÃ§Ãµes

Este projeto estÃ¡ em constante evoluÃ§Ã£o. Sinta-se Ã  vontade para:

- ğŸ› **Reportar bugs** abrindo uma issue
- ğŸ’¡ **Sugerir melhorias** atravÃ©s de issues
- ğŸ”§ **Contribuir com cÃ³digo** enviando pull requests

Toda colaboraÃ§Ã£o Ã© bem-vinda para tornar o UNESPedia ainda mais Ãºtil para a comunidade acadÃªmica!

---

<div align="center">
  <p><strong>Desenvolvido para a disciplina de Aprendizado Profundo - PPGCC Unesp</strong></p>
  <p>Feito com ğŸ’™</a></p>
</div>
