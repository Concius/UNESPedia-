# llm_handler.py



import streamlit as st
import google.generativeai as genai
from config_loader import carregar_config

# Carrega a configura√ß√£o no in√≠cio do m√≥dulo
config = carregar_config()
llm_config = config['llm_defaults']

def gerar_resposta_com_llm(contexto, pergunta, api_key, nomes_ficheiros, historico_chat, debug_mode=False, 
                           temperature=llm_config['temperature'], 
                           top_p=llm_config['top_p'], 
                           top_k=llm_config['top_k'], 
                           max_output_tokens=llm_config['max_output_tokens']):
    """Envia o prompt para o Gemini com par√¢metros configur√°veis."""
    try:
        genai.configure(api_key=api_key)
        
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            max_output_tokens=max_output_tokens,
        )
        
        #Verifica√ß√£o do que est√° sendo enviado para a API
        st.caption(f"üîß **Config enviado para API:** T={temperature}, P={top_p}, K={top_k}, Max={max_output_tokens}")
        
        model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)
        
        if debug_mode:
            st.write("üìÅ DEBUG: Arquivos que foram processados:")
            for i, nome in enumerate(nomes_ficheiros):
                st.write(f"  {i+1}. {nome}")
        
        # Formata o hist√≥rico do chat
        historico_formatado = "\\n".join([f"{msg['role']}: {msg['content']}" for msg in historico_chat])
        
        if debug_mode:
            st.write(f"üí¨ DEBUG: Hist√≥rico tem {len(historico_chat)} mensagens")
            st.write(f"üéõÔ∏è DEBUG: Par√¢metros do LLM - Temperature: {temperature}, Top-p: {top_p}, Top-k: {top_k}, Max tokens: {max_output_tokens}")

        prompt = f"""
        **Instru√ß√µes:** Voc√™ √© um assistente de pesquisa. Responda √† "√öltima pergunta do usu√°rio" baseando-se no "Contexto" e no "Hist√≥rico da Conversa".
        Os ficheiros carregados pelo usu√°rio s√£o: {', '.join(nomes_ficheiros)}.

        **Hist√≥rico da Conversa:**
        {historico_formatado}

        **Contexto extra√≠do dos documentos (use para basear a sua resposta):**
        ---
        {contexto}
        ---

        **√öltima pergunta do usu√°rio:** {pergunta}

        **Sua Resposta:**
        """
        
        if debug_mode:
            with st.expander("ü§ñ DEBUG: Prompt enviado para o LLM"):
                st.text(prompt)
                st.write(f"üìè Tamanho total do prompt: {len(prompt)} caracteres")
        
        resposta = model.generate_content(prompt)
        
        # NOVO: Ap√≥s gerar a resposta, mostrar metadados se dispon√≠vel
        if hasattr(resposta, 'usage_metadata'):
            st.caption(f"üìä **Usage API:** {resposta.usage_metadata}")
        
        if debug_mode:
            st.write(f"‚úÖ DEBUG: LLM respondeu com {len(resposta.text)} caracteres")
            # Tentar mostrar mais detalhes da resposta
            if hasattr(resposta, 'candidates'):
                st.write(f"üîç DEBUG: N√∫mero de candidatos: {len(resposta.candidates)}")
        
        return resposta.text
    except Exception as e:
        st.error(f"Erro ao comunicar com a API do Gemini: {e}")
        return "Desculpe, ocorreu um erro ao tentar gerar a resposta."